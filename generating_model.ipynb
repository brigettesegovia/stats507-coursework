{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: Hugging Face "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses : [https://huggingface.co/docs/datasets/en/quickstart#vision]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pip intalling necessary items\n",
    "# ! pip install datasets\n",
    "# ! pip install datasets[vision] #used to work with the Image features \n",
    "\n",
    "# # I will be using Pytorch for this project, so I will install it here. But you can use Tensorflow if you prefer\n",
    "# ! pip install torch\n",
    "# ! pip install torchvision\n",
    "\n",
    "\n",
    "# # I will be using the Hugging Face Transformers library for this project\n",
    "# ! pip install transformers[torch]\n",
    "# ! pip install \"accelerate>=0.26.0\"\n",
    "# ! pip install transformers\n",
    "# ! pip install evaluate\n",
    "# ! pip install Pillow\n",
    "# ! pip install evaluate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import evaluate\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import load_dataset, Image\n",
    "from transformers import AutoImageProcessor\n",
    "from torchvision.transforms import Compose, ColorJitter, ToTensor,  RandomResizedCrop, Normalize\n",
    "from transformers import DefaultDataCollator\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification, ViTImageProcessor, SwinForImageClassification\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the Beans dataset \n",
    "dataset = load_dataset(\"beans\")\n",
    "# dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'image_file_path': Value(dtype='string', id=None),\n",
       "  'image': Image(mode=None, decode=True, id=None),\n",
       "  'labels': ClassLabel(names=['angular_leaf_spot', 'bean_rust', 'healthy'], id=None)},)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].features, #dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a way to get the image features easier for model \n",
    "labels = dataset['train'].features['labels']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A feature extractor is needed to preprocess the image into a TENSOR\n",
    "# checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "checkpoint = \"microsoft/swin-tiny-patch4-window7-224\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(example):\n",
    "    inputs = image_processor(example['image'], return_tensors='pt')\n",
    "    inputs['labels'] = example['labels']\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(example_batch):\n",
    "    # Take a list of PIL images and turn them to pixel values\n",
    "    inputs = image_processor([x for x in example_batch['image']], return_tensors='pt')\n",
    "\n",
    "    # Don't forget to include the labels!\n",
    "    inputs['labels'] = example_batch['labels']\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_ds = dataset.with_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[[-1.1760, -1.1760, -1.1760,  ..., -0.0458,  0.6563, -0.5253],\n",
       "          [-1.2617, -1.2445, -1.2274,  ...,  0.0569,  0.1083, -0.5938],\n",
       "          [-1.3130, -1.3302, -1.2959,  ..., -0.4397, -0.7650, -0.4226],\n",
       "          ...,\n",
       "          [-1.1075, -1.1247, -1.1932,  ..., -0.0801, -0.0801, -0.2342],\n",
       "          [-1.1589, -1.2103, -1.2103,  ...,  0.8447,  0.4508,  0.3652],\n",
       "          [-1.1932, -1.2274, -1.2617,  ...,  0.6563,  0.3652,  0.5193]],\n",
       "\n",
       "         [[-1.3880, -1.3880, -1.4755,  ..., -0.6702, -0.0049, -1.2129],\n",
       "          [-1.4405, -1.4405, -1.5105,  ..., -0.5301, -0.4076, -1.1429],\n",
       "          [-1.4755, -1.4755, -1.5280,  ..., -0.9678, -1.3179, -1.0028],\n",
       "          ...,\n",
       "          [-1.5105, -1.5455, -1.6155,  ..., -0.3375,  0.0476,  0.3452],\n",
       "          [-1.5805, -1.6155, -1.6155,  ...,  0.6954,  0.9930,  1.0805],\n",
       "          [-1.6155, -1.6331, -1.6506,  ...,  0.3102,  0.8529,  1.0980]],\n",
       "\n",
       "         [[-1.7870, -1.7870, -1.8044,  ..., -0.8284, -0.2532, -1.2467],\n",
       "          [-1.7870, -1.7870, -1.8044,  ..., -0.8110, -0.7064, -1.2293],\n",
       "          [-1.7522, -1.7870, -1.8044,  ..., -1.0201, -1.2641, -0.9678],\n",
       "          ...,\n",
       "          [-1.4384, -1.4907, -1.5779,  ..., -0.8807, -0.9156, -0.8981],\n",
       "          [-1.4907, -1.5604, -1.5953,  ...,  0.0256, -0.1138, -0.1661],\n",
       "          [-1.5430, -1.5953, -1.6476,  ..., -0.1835,  0.0082,  0.1999]]],\n",
       "\n",
       "\n",
       "        [[[-1.0733, -1.0048, -0.8335,  ...,  0.0398,  0.1083, -0.4054],\n",
       "          [-1.0219, -0.9363, -0.8335,  ..., -0.0629,  0.0741, -0.4739],\n",
       "          [-0.9534, -1.0219, -1.0904,  ..., -0.0801,  0.1083, -0.6109],\n",
       "          ...,\n",
       "          [-1.0219, -1.0219, -1.0904,  ..., -0.3541, -0.3883, -0.8164],\n",
       "          [-1.1589, -1.1075, -1.0390,  ..., -0.3369, -0.3369, -0.8335],\n",
       "          [-1.0904, -1.1247, -1.1418,  ..., -0.1999, -0.2513, -0.5253]],\n",
       "\n",
       "         [[-0.2150, -0.2500, -0.1625,  ...,  0.7479,  0.6779, -0.0574],\n",
       "          [-0.1800, -0.1800, -0.1275,  ...,  0.6954,  0.7479, -0.0399],\n",
       "          [-0.1275, -0.2500, -0.3550,  ...,  0.6604,  0.8179, -0.1625],\n",
       "          ...,\n",
       "          [-1.5280, -1.4755, -1.4930,  ..., -0.7052, -0.8102, -1.2829],\n",
       "          [-1.5980, -1.5630, -1.4930,  ..., -0.7402, -0.7752, -1.3179],\n",
       "          [-1.5105, -1.5980, -1.6155,  ..., -0.6176, -0.7227, -1.0203]],\n",
       "\n",
       "         [[-1.2119, -1.5081, -1.5081,  ..., -0.2532, -0.1138, -0.8633],\n",
       "          [-1.2990, -1.4559, -1.4036,  ..., -0.3055, -0.1312, -0.9156],\n",
       "          [-1.4036, -1.5256, -1.5081,  ..., -0.3927, -0.0267, -0.8458],\n",
       "          ...,\n",
       "          [-0.7413, -0.7064, -0.7761,  ..., -0.0790, -0.1312, -0.6193],\n",
       "          [-0.8284, -0.7936, -0.7413,  ..., -0.0441, -0.0267, -0.6193],\n",
       "          [-0.7413, -0.8110, -0.8458,  ...,  0.0953,  0.0256, -0.3230]]]]), 'labels': [0, 0]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_ds['train'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['labels'] for x in batch])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat a function that passses the predictions and labels to the accuracy function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForImageClassification\n",
    "import torch.nn as nn\n",
    "\n",
    "labels = dataset['train'].features['labels'].names\n",
    "\n",
    "# Load the model with ignore_mismatched_sizes to bypass the size mismatch issue\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=len(labels),\n",
    "    id2label={str(i): c for i, c in enumerate(labels)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)},\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Create a new classifier layer with the correct number of labels\n",
    "model.classifier = nn.Linear(in_features=model.classifier.in_features, out_features=len(labels))\n",
    "\n",
    "# Now, model is ready with the correct classifier layer for your dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/lib/python3.12/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda/lib/python3.12/site-packages/transformers/training_args.py:1583: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./vit-base-beans\",\n",
    "    per_device_train_batch_size=16,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=4,\n",
    "    fp16=False,  # Ensure fp16 is disabled\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=True,\n",
    "    report_to='tensorboard',\n",
    "    load_best_model_at_end=True,\n",
    "    no_cuda=True,  # Force the use of CPU\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6w/4l8xsqjn63b50789db29p6tm0000gr/T/ipykernel_280/3482212076.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_ds[\"train\"],\n",
    "    eval_dataset=prepared_ds[\"validation\"],\n",
    "    tokenizer=image_processor,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b31b4b4fe2f4fd5a15c2e3370a0477a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/260 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9649, 'grad_norm': 15.253263473510742, 'learning_rate': 0.00019230769230769233, 'epoch': 0.15}\n",
      "{'loss': 0.4952, 'grad_norm': 12.642136573791504, 'learning_rate': 0.00018461538461538463, 'epoch': 0.31}\n",
      "{'loss': 0.3586, 'grad_norm': 6.72530460357666, 'learning_rate': 0.00017692307692307693, 'epoch': 0.46}\n",
      "{'loss': 0.4554, 'grad_norm': 36.84693908691406, 'learning_rate': 0.00016923076923076923, 'epoch': 0.62}\n",
      "{'loss': 0.3472, 'grad_norm': 8.32830810546875, 'learning_rate': 0.00016153846153846155, 'epoch': 0.77}\n",
      "{'loss': 0.2445, 'grad_norm': 12.446823120117188, 'learning_rate': 0.00015384615384615385, 'epoch': 0.92}\n",
      "{'loss': 0.2426, 'grad_norm': 9.914612770080566, 'learning_rate': 0.00014615384615384615, 'epoch': 1.08}\n",
      "{'loss': 0.1964, 'grad_norm': 9.404363632202148, 'learning_rate': 0.00013846153846153847, 'epoch': 1.23}\n",
      "{'loss': 0.0612, 'grad_norm': 3.2337546348571777, 'learning_rate': 0.00013076923076923077, 'epoch': 1.38}\n",
      "{'loss': 0.1068, 'grad_norm': 37.80059051513672, 'learning_rate': 0.0001230769230769231, 'epoch': 1.54}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e7f87ca35e41019e2bc8ef9648b837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06387340277433395, 'eval_accuracy': 0.9774436090225563, 'eval_runtime': 5.761, 'eval_samples_per_second': 23.086, 'eval_steps_per_second': 2.951, 'epoch': 1.54}\n",
      "{'loss': 0.1322, 'grad_norm': 35.027339935302734, 'learning_rate': 0.00011538461538461538, 'epoch': 1.69}\n",
      "{'loss': 0.144, 'grad_norm': 0.8483041524887085, 'learning_rate': 0.0001076923076923077, 'epoch': 1.85}\n",
      "{'loss': 0.104, 'grad_norm': 0.03202478587627411, 'learning_rate': 0.0001, 'epoch': 2.0}\n",
      "{'loss': 0.0698, 'grad_norm': 0.2923188805580139, 'learning_rate': 9.230769230769232e-05, 'epoch': 2.15}\n",
      "{'loss': 0.0238, 'grad_norm': 0.008691404946148396, 'learning_rate': 8.461538461538461e-05, 'epoch': 2.31}\n"
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b41d19d1d547eba4b5e2e45e37f3cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        4.0\n",
      "  eval_accuracy           =        1.0\n",
      "  eval_loss               =     0.0043\n",
      "  eval_runtime            = 0:00:04.59\n",
      "  eval_samples_per_second =     28.974\n",
      "  eval_steps_per_second   =      3.703\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "# some nice to haves:\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
